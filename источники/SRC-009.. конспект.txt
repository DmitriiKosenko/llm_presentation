История LLM-агентов: 10 ярких моментов

https://habr.com/en/companies/friflex/articles/822665/

--------------------------------------------------------------------------------

Как писал Пелевин, «в сущности, функция LLM – это доведенное до немыслимого совершенства автозаполнение. LLM не думает. Она тренируется на огромном корпусе созданных прежде текстов «…» и на этой основе предсказывает, как будет расти и развиваться новая последовательность слов, и как она, вероятней всего, развиваться не будет… Это похоже на процесс формирования юного члена общества на основе ежедневно поступающих вербальных инструкций, подзатыльников и наблюдения за тем, кому дают еду, а кому нет». 

LLM — это нейронная сеть, обученная на большом объеме текста. Она может анализировать, понимать и генерировать тексты на естественном языке, использовать обширные базы данных и понимать контекст. 

Как развивалась идея о том, что машину можно научить понимать и создавать текст, как будто это пишет человек, расскажу дальше. 

1957: Фрэнк Розэнблатт создает перцептрон

В середине прошлого века американский ученый Розэнблатт увлеченно изучал человеческий мозг. Он мечтал создать искусственную модель, которая могла бы имитировать его способность обучаться и распознавать паттерны.

«Рассказы о машинах, обладающих человеческими качествами, давно захватили научную фантастику. Но мы собираемся стать свидетелями рождения такой машины — машины, способной воспринимать, распознавать и идентифицировать свое окружение без обучения или контроля со стороны человека», — писал Розэнблатт в 1958. 

Свои идеи ученый изложил в работе «Принцип перцептрона». Перцептроном он назвал устройство, которое моделировало процесс человеческого восприятия. Это была простая модель искусственной нейронной сети. В 1960 году Розэнблат показал, как она работает, на первом в истории нейрокомпьютере «Марк-1».

У этого устройства был своеобразный глаз — матрица фоточувствительных элементов. «Марк-1» умел распознавать некоторые английские буквы и геометрические формы, которые Розэнблатт показывал ему на карточках или на бумаге. Кроме того, компьютер мог изменять весовые коэффициенты связей, чтобы улучшать распознавание после обратной связи о результате. 



1965: Джозеф Вайценбаум создает Элизу

Первая система обработки естественного языка

Пока Розэнблатт разрабатывал перцептрон, другой американский ученый, Джозеф Вайценбаум, работал над программой Элиза. Элиза была простым чат-ботом. Она анализировала предложения, которые вводил пользователь, и находила ключевые слова.

программа показала, что компьютер может участвовать в осмысленных диалогах на естественном языке. 



1970: Марвин Минский и Сеймур Паперт публикуют книгу «Перцептроны»

Начинается «зима искусственного интеллекта»

Минский вместе с Папертом исследовали математические свойства перцептрона и показали, что он не способен решать целый ряд задач, связанных с инвариантным представлением. Например, читать буквы или цифры, которые по-разному расположены на странице.

Их книга «Перцетроны» вышла в 1970. После публикации интерес к исследованию нейросетей упал настолько, что семидесятые стали называть «зимой искусственного интеллекта». Переместился не только научный интерес, но и субсидии американских правительственных организаций — к радости последователей символьного подхода. 



1986: Дэвид Румельхарт и Джеффри Хинтон предложили метод обратного распространения ошибки 

Критика перцептрона привела не только к «зиме искусственного интеллекта»: исследователи стали искать более мощные модели. На смену однослойному перцептрону Розэнблатта пришел многослойный.

Математический аппарат метода обратного распространения ошибки довольно прост. Но он позволил нейронным сетям обучаться на данных значительно сложнее и разнообразнее, чем раньше.


1997: IBM Deep Blue побеждает чемпиона мира по шахматам Гарри Каспарова

Победа Deep Blue показала, что машины могут превзойти человека в интеллектуально сложных задачах, а еще продемонстрировала возможности машинного обучения и анализа больших данных. Это событие стало источником вдохновения для дальнейших исследований искусственного интеллекта.


2007: IBM представила систему Watson

Watson еще не был LLM-агентом в современном понимании. Но его архитектура уже включала в себя различные методы анализа и обработки естественного языка. Например, компьютер делил текст на отдельные слова и фразы, а затем преобразовывал их в токены. 


2017: инженеры Google описали архитектуру Transformer

Transformer радикально изменила подход к обработке естественного языка и NLP

Ашиш Васвани и его коллеги опубликовали культовую статью Attention Is All You Need (Внимание — все, что вам нужно) и рассказали в ней про архитектуру нейронной сети под названием Transformer.

О значении Transformer в истории LLM-агентов можно говорить много. Но достаточно будет, что сегодня это доминирующая архитектура нейронных сетей для NLP.  


2018: OpenAI представила GPT-1

Проще обучать модели на неаннотированных данных

Если предварительно обучать модель на разнообразном корпусе неаннотированного текста, она сможет значительно лучше понимать естественный язык, отвечать на вопросы, оценивать семантическую близость и выполнять другие подобные задачи.

Архитектура Transformer помогает модели лучше справляться с долгосрочными зависимостями в тексте.

Универсальная модель превосходит модели с архитектурой, адаптированной для каждой задачи, и обученные дискриминативным методом


2019: Появилась VL-BERT

Модель научилась обрабатывать информацию как из текста, так и из изображений

VL-BERT — одна из первых мультимодальных LLM-моделей. Она интересна тем, что предварительно обучается на масштабных визуально-лингвистических. Это позволяет ей синтезировать информацию из области интереса в изображении и связанные текстовые описания, чтобы сформировать обоснование выбора ответа. То есть, она не только распознает, что на изображении, но и связывает это с текстом.


2020: OpenAI представила GPT-3 

Модель научилась генерировать тексты, как человек.

Она умела как выполнять машинный перевод, так и писать код. Поэтому круг применения у нее был значительно шире, чем у предшественниц.

Модель GPT-3 развивалась, и в результате появился ChatGPT. Это версия модели, которая обучалась не только на текстах из интернета, но и на диалоговых данных. Если до появления ChatGPT языковые модели все еще по большей части были объектом теоретических исследований, то теперь их стали широко применять на практике.

